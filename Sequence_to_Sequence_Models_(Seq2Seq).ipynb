{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyP3dCP/XqXKePE/nFWdR16y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/State-of-the-Art/blob/main/Sequence_to_Sequence_Models_(Seq2Seq).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFAuogvJqvJg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hidden_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.rnn = nn.LSTM(emb_dim, hidden_dim, n_layers, dropout=dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src):\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        outputs, (hidden, cell) = self.rnn(embedded)\n",
        "        return hidden, cell\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hidden_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.rnn = nn.LSTM(emb_dim, hidden_dim, n_layers, dropout=dropout)\n",
        "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, trg, hidden, cell):\n",
        "        trg = trg.unsqueeze(0)\n",
        "        embedded = self.dropout(self.embedding(trg))\n",
        "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
        "        prediction = self.fc_out(output.squeeze(0))\n",
        "        return prediction, hidden, cell\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        hidden, cell = self.encoder(src)\n",
        "        output, hidden, cell = self.decoder(trg[0], hidden, cell)\n",
        "        return output\n",
        "\n",
        "# Hyperparameters\n",
        "INPUT_DIM = 100\n",
        "OUTPUT_DIM = 100\n",
        "ENC_EMB_DIM = 32\n",
        "DEC_EMB_DIM = 32\n",
        "HIDDEN_DIM = 64\n",
        "N_LAYERS = 2\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "# Instantiate the encoder and decoder\n",
        "encoder = Encoder(INPUT_DIM, ENC_EMB_DIM, HIDDEN_DIM, N_LAYERS, ENC_DROPOUT)\n",
        "decoder = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HIDDEN_DIM, N_LAYERS, DEC_DROPOUT)\n",
        "\n",
        "# Create Seq2Seq model\n",
        "model = Seq2Seq(encoder, decoder)\n",
        "\n",
        "# Define loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "encoder_optimizer = optim.Adam(encoder.parameters(), lr=0.01)\n",
        "decoder_optimizer = optim.Adam(decoder.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop for Seq2Seq model\n",
        "def train(seq2seq_model, iterator, criterion, enc_optimizer, dec_optimizer, clip):\n",
        "    seq2seq_model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for batch in iterator:\n",
        "        src, trg = batch\n",
        "        enc_optimizer.zero_grad()\n",
        "        dec_optimizer.zero_grad()\n",
        "\n",
        "        output = seq2seq_model(src, trg)\n",
        "\n",
        "        loss = criterion(output, trg[1])\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(seq2seq_model.parameters(), clip)\n",
        "\n",
        "        enc_optimizer.step()\n",
        "        dec_optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "# Example usage with dummy data\n",
        "dummy_iterator = [(\n",
        "    torch.randint(0, INPUT_DIM, (10,)),\n",
        "    torch.randint(0, OUTPUT_DIM, (10,))\n",
        ") for _ in range(100)]\n",
        "\n",
        "# Train for one epoch\n",
        "train_loss = train(model, dummy_iterator, criterion, encoder_optimizer, decoder_optimizer, clip=1)\n",
        "print(f\"Training Loss: {train_loss:.4f}\")"
      ]
    }
  ]
}