{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyODFjQSAQwP+WPmSyHxr1Mo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/State-of-the-Art/blob/main/Complete_LSTM_Training_Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eKg2bJXH54aq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# --- Model ---\n",
        "class SimpleLSTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(SimpleLSTM, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        return self.fc(lstm_out[:, -1, :])  # Last time step\n",
        "\n",
        "# --- Hyperparameters ---\n",
        "seq_length = 5\n",
        "batch_size = 16\n",
        "input_dim = 4\n",
        "hidden_dim = 32\n",
        "output_dim = 2\n",
        "num_epochs = 10\n",
        "lr = 0.001\n",
        "\n",
        "# --- Synthetic Data ---\n",
        "X = torch.rand(500, seq_length, input_dim)\n",
        "y = torch.randint(0, output_dim, (500,))  # Class indices for classification\n",
        "\n",
        "dataset = TensorDataset(X, y)\n",
        "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# --- Model, Loss, Optimizer ---\n",
        "model = SimpleLSTM(input_dim, hidden_dim, output_dim)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "# --- Training Loop ---\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for batch_x, batch_y in loader:\n",
        "        output = model(batch_x)\n",
        "        loss = criterion(output, batch_y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(loader)\n",
        "    print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# --- Output Shape Check ---\n",
        "with torch.no_grad():\n",
        "    sample_input = torch.rand(batch_size, seq_length, input_dim)\n",
        "    sample_output = model(sample_input)\n",
        "    print(\"LSTM Output Shape:\", sample_output.shape)  # Should be [batch_size, output_dim]"
      ]
    }
  ]
}