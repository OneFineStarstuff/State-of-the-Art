{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNxfcSW4kfaBq84Wz9SoAhi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/State-of-the-Art/blob/main/Transformer_Variants.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DwrPXb9aYds1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from einops.layers.torch import Rearrange\n",
        "\n",
        "# Define the SimpleViT class\n",
        "class SimpleViT(nn.Module):\n",
        "    def __init__(self, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim):\n",
        "        super(SimpleViT, self).__init__()\n",
        "        assert image_size % patch_size == 0, \"Image size must be divisible by patch size.\"\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = (image_size // patch_size) ** 2\n",
        "        self.dim = dim\n",
        "\n",
        "        self.to_patch_embedding = nn.Sequential(\n",
        "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=patch_size, p2=patch_size),  # Rearrange patches\n",
        "            nn.Linear(patch_size * patch_size * 3, dim)  # Linear embedding\n",
        "        )\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))  # Class token\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_patches + 1, dim))  # Positional embedding\n",
        "\n",
        "        self.transformer = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(dim, heads, mlp_dim), num_layers=depth\n",
        "        )  # Transformer encoder\n",
        "\n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.LayerNorm(dim),\n",
        "            nn.Linear(dim, num_classes)  # Classification head\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        x = self.to_patch_embedding(img)  # Convert image to patch embeddings\n",
        "        b, n, _ = x.shape\n",
        "\n",
        "        cls_tokens = self.cls_token.expand(b, -1, -1)  # Expand class token for each batch\n",
        "        x = torch.cat((cls_tokens, x), dim=1)  # Concatenate class token with patch embeddings\n",
        "        x += self.pos_embedding[:, :(n + 1)]  # Add positional embedding\n",
        "\n",
        "        x = self.transformer(x)  # Pass through transformer encoder\n",
        "        x = x[:, 0]  # Select the class token\n",
        "        return self.mlp_head(x)  # Pass through classification head\n",
        "\n",
        "# Example usage\n",
        "model = SimpleViT(image_size=32, patch_size=8, num_classes=10, dim=128, depth=6, heads=8, mlp_dim=256)  # Instantiate the model\n",
        "img = torch.randn(64, 3, 32, 32)  # Example input image batch (batch_size=64, channels=3, height=32, width=32)\n",
        "output = model(img)\n",
        "\n",
        "# Print the shape of the output\n",
        "print(output.shape)  # Expected shape: [64, 10]"
      ]
    }
  ]
}