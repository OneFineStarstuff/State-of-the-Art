{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOANS0cznnz456e5GvP6hEX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/State-of-the-Art/blob/main/Few_Shot_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJMG2wTrUcoS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ProtoNet(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim):\n",
        "        super(ProtoNet, self).__init__()\n",
        "        self.embedding = nn.Sequential(\n",
        "            nn.Conv2d(input_dim, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(128 * 7 * 7, embedding_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.embedding(x)\n",
        "\n",
        "def euclidean_distance(a, b):\n",
        "    return torch.cdist(a, b)\n",
        "\n",
        "def compute_prototypes(embeddings, labels, n_classes):\n",
        "    prototypes = []\n",
        "    for i in range(n_classes):\n",
        "        class_embeddings = embeddings[labels == i]\n",
        "        prototype = class_embeddings.mean(dim=0)\n",
        "        prototypes.append(prototype)\n",
        "    return torch.stack(prototypes)\n",
        "\n",
        "# Example usage\n",
        "input_dim = 1  # e.g., for grayscale images\n",
        "embedding_dim = 64\n",
        "n_classes = 5\n",
        "model = ProtoNet(input_dim, embedding_dim)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(100):\n",
        "    optimizer.zero_grad()\n",
        "    support_set = torch.randn(25, input_dim, 28, 28)  # 5 classes, 5 samples each\n",
        "    support_labels = torch.tensor([i for i in range(n_classes) for _ in range(5)])\n",
        "    query_set = torch.randn(10, input_dim, 28, 28)  # 2 samples per class for query\n",
        "    query_labels = torch.tensor([i for i in range(n_classes) for _ in range(2)])\n",
        "\n",
        "    support_embeddings = model(support_set)\n",
        "    query_embeddings = model(query_set)\n",
        "\n",
        "    prototypes = compute_prototypes(support_embeddings, support_labels, n_classes)\n",
        "    distances = euclidean_distance(query_embeddings, prototypes)\n",
        "    loss = criterion(-distances, query_labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f'Epoch {epoch + 1}, Loss: {loss.item():.4f}')"
      ]
    }
  ]
}