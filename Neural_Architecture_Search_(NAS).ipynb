{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNlN2fuaanYEuaUsj3VPI7w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/State-of-the-Art/blob/main/Neural_Architecture_Search_(NAS).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZX6Np_xUJXUq"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class RandomNet(nn.Module):\n",
        "    def __init__(self, layer_sizes):\n",
        "        super(RandomNet, self).__init__()\n",
        "        layers = []\n",
        "        for i in range(len(layer_sizes) - 1):\n",
        "            layers.append(nn.Linear(layer_sizes[i], layer_sizes[i+1]))\n",
        "            if i < len(layer_sizes) - 2:\n",
        "                layers.append(nn.ReLU())\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "def random_search(input_dim, output_dim, search_space, train_data, train_labels, epochs=5):\n",
        "    best_model = None\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    for i in range(search_space['num_models']):\n",
        "        layer_sizes = [input_dim] + random.sample(search_space['hidden_sizes'], k=random.randint(1, search_space['max_layers'])) + [output_dim]\n",
        "        model = RandomNet(layer_sizes)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            optimizer.zero_grad()\n",
        "            output = model(train_data)\n",
        "            loss = nn.MSELoss()(output, train_labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        if loss.item() < best_loss:\n",
        "            best_loss = loss.item()\n",
        "            best_model = model\n",
        "\n",
        "    return best_model, best_loss\n",
        "\n",
        "# Example usage\n",
        "search_space = {\n",
        "    'num_models': 10,\n",
        "    'hidden_sizes': [16, 32, 64, 128],\n",
        "    'max_layers': 3\n",
        "}\n",
        "\n",
        "train_data = torch.randn(100, 10)\n",
        "train_labels = torch.randn(100, 1)\n",
        "\n",
        "best_model, best_loss = random_search(10, 1, search_space, train_data, train_labels)\n",
        "print(f'Best Loss: {best_loss:.4f}')"
      ]
    }
  ]
}