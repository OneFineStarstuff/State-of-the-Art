{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMZCvAp4uesv1HHTnidPjgI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/State-of-the-Art/blob/main/Twin_Delayed_Deep_Deterministic_Policy_Gradient_(TD3).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "olqnbwe0-_Fj"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import random\n",
        "\n",
        "# Define Actor network\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, max_action):\n",
        "        super(Actor, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(state_dim, 400),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(400, 300),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(300, action_dim),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "        self.max_action = max_action\n",
        "\n",
        "    def forward(self, state):\n",
        "        return self.max_action * self.fc(state)\n",
        "\n",
        "# Define Critic network (Q-function)\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(Critic, self).__init__()\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Linear(state_dim + action_dim, 400),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(400, 300),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(300, 1)\n",
        "        )\n",
        "        self.fc2 = nn.Sequential(\n",
        "            nn.Linear(state_dim + action_dim, 400),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(400, 300),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(300, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        sa = torch.cat([state, action], dim=1)\n",
        "        q1 = self.fc1(sa)\n",
        "        q2 = self.fc2(sa)\n",
        "        return q1, q2\n",
        "\n",
        "# TD3 Agent\n",
        "class TD3:\n",
        "    def __init__(self, state_dim, action_dim, max_action, env):\n",
        "        self.actor = Actor(state_dim, action_dim, max_action)\n",
        "        self.actor_target = Actor(state_dim, action_dim, max_action)\n",
        "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=3e-4)\n",
        "\n",
        "        self.critic = Critic(state_dim, action_dim)\n",
        "        self.critic_target = Critic(state_dim, action_dim)\n",
        "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=3e-4)\n",
        "\n",
        "        self.max_action = max_action\n",
        "        self.replay_buffer = deque(maxlen=1000000)\n",
        "        self.batch_size = 100\n",
        "        self.discount = 0.99\n",
        "        self.tau = 0.005\n",
        "        self.policy_noise = 0.2\n",
        "        self.noise_clip = 0.5\n",
        "        self.policy_freq = 2\n",
        "        self.total_it = 0\n",
        "        self.env = env\n",
        "\n",
        "    def select_action(self, state):\n",
        "        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
        "        return self.actor(state).data.numpy().flatten()\n",
        "\n",
        "    def train(self):\n",
        "        for _ in range(200):  # Number of updates per iteration\n",
        "            self.total_it += 1\n",
        "\n",
        "            # Sample a batch of transitions from replay buffer\n",
        "            batch = random.sample(self.replay_buffer, self.batch_size)\n",
        "            states, actions, rewards, next_states, dones = zip(*batch)\n",
        "\n",
        "            states = torch.tensor(states, dtype=torch.float32)\n",
        "            actions = torch.tensor(actions, dtype=torch.float32)\n",
        "            rewards = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1)\n",
        "            next_states = torch.tensor(next_states, dtype=torch.float32)\n",
        "            dones = torch.tensor(dones, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                noise = (torch.randn_like(actions) * self.policy_noise).clamp(-self.noise_clip, self.noise_clip)\n",
        "                next_actions = (self.actor_target(next_states) + noise).clamp(-self.max_action, self.max_action)\n",
        "                target_Q1, target_Q2 = self.critic_target(next_states, next_actions)\n",
        "                target_Q = rewards + self.discount * torch.min(target_Q1, target_Q2) * (1 - dones)\n",
        "\n",
        "            current_Q1, current_Q2 = self.critic(states, actions)\n",
        "            critic_loss = nn.MSELoss()(current_Q1, target_Q) + nn.MSELoss()(current_Q2, target_Q)\n",
        "\n",
        "            self.critic_optimizer.zero_grad()\n",
        "            critic_loss.backward()\n",
        "            self.critic_optimizer.step()\n",
        "\n",
        "            if self.total_it % self.policy_freq == 0:\n",
        "                actor_loss = -self.critic(states, self.actor(states))[0].mean()\n",
        "                self.actor_optimizer.zero_grad()\n",
        "                actor_loss.backward()\n",
        "                self.actor_optimizer.step()\n",
        "\n",
        "                # Update the frozen target models\n",
        "                for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "                    target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
        "\n",
        "                for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "                    target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
        "\n",
        "    def add_to_replay_buffer(self, state, action, reward, next_state, done):\n",
        "        self.replay_buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def run(self, max_episodes=100):\n",
        "        for episode in range(max_episodes):\n",
        "            state = self.env.reset()\n",
        "            episode_reward = 0\n",
        "            done = False\n",
        "\n",
        "            while not done:\n",
        "                action = self.select_action(state)\n",
        "                next_state, reward, done, truncated, info = self.env.step(action)  # Handle new step API\n",
        "                self.add_to_replay_buffer(state, action, reward, next_state, done or truncated)\n",
        "                state = next_state\n",
        "                episode_reward += reward\n",
        "\n",
        "            print(f\"Episode {episode}, Reward: {episode_reward}\")\n",
        "            if len(self.replay_buffer) > self.batch_size:\n",
        "                self.train()\n",
        "\n",
        "env = gym.make(\"Pendulum-v1\", new_step_api=True)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "max_action = float(env.action_space.high[0])\n",
        "\n",
        "agent = TD3(state_dim, action_dim, max_action, env)\n",
        "agent.run()"
      ]
    }
  ]
}