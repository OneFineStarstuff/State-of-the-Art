{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNarKGjtPIyCtXuvVjf525s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/State-of-the-Art/blob/main/Capsule_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBp0sbCHNPnU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CapsuleLayer(nn.Module):\n",
        "    def __init__(self, num_capsules, num_routes, in_dim, out_dim):\n",
        "        super(CapsuleLayer, self).__init__()\n",
        "        self.num_capsules = num_capsules\n",
        "        self.num_routes = num_routes\n",
        "        self.capsules = nn.ModuleList([\n",
        "            nn.Linear(in_dim, out_dim) for _ in range(num_capsules)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        u_hat = torch.stack([capsule(x) for capsule in self.capsules], dim=1)  # [batch_size, num_capsules, out_dim]\n",
        "        b = torch.zeros(x.size(0), self.num_capsules, self.num_routes, device=x.device)  # [batch_size, num_capsules, num_routes]\n",
        "\n",
        "        for i in range(3):  # Routing iterations\n",
        "            c = F.softmax(b, dim=2)  # [batch_size, num_capsules, num_routes]\n",
        "            s = (c.unsqueeze(3) * u_hat.unsqueeze(2)).sum(dim=1)  # [batch_size, num_routes, out_dim]\n",
        "            v = self.squash(s)  # [batch_size, num_routes, out_dim]\n",
        "            b = b + (u_hat.unsqueeze(2) * v.unsqueeze(1)).sum(dim=-1)  # [batch_size, num_capsules, num_routes]\n",
        "\n",
        "        return v\n",
        "\n",
        "    @staticmethod\n",
        "    def squash(s, dim=-1):\n",
        "        s_squared_norm = (s ** 2).sum(dim=dim, keepdim=True)  # [batch_size, num_routes, 1]\n",
        "        scale = s_squared_norm / (1 + s_squared_norm)  # Scaling factor\n",
        "        return scale * s / torch.sqrt(s_squared_norm + 1e-8)  # Apply squash activation\n",
        "\n",
        "# Example usage\n",
        "input_data = torch.randn(32, 8)  # Batch size of 32, input dimension 8\n",
        "capsule_layer = CapsuleLayer(num_capsules=10, num_routes=8, in_dim=8, out_dim=16)\n",
        "output = capsule_layer(input_data)\n",
        "\n",
        "# Print the shape of the capsule output\n",
        "print(\"Capsule output shape:\", output.shape)  # Expected shape: [32, 8, 16]"
      ]
    }
  ]
}