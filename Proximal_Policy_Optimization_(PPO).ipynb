{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNv/RG8ij8gLDuSy/4V3PIi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/State-of-the-Art/blob/main/Proximal_Policy_Optimization_(PPO).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8IvidN1ZMHZr"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from torch.distributions import MultivariateNormal\n",
        "\n",
        "# Actor-Critic network\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(ActorCritic, self).__init__()\n",
        "        self.actor = nn.Sequential(\n",
        "            nn.Linear(state_dim, 64),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(64, action_dim)\n",
        "        )\n",
        "        self.critic = nn.Sequential(\n",
        "            nn.Linear(state_dim, 64),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "        self.log_std = nn.Parameter(torch.zeros(action_dim))\n",
        "\n",
        "    def forward(self, state):\n",
        "        action_mean = self.actor(state)\n",
        "        action_std = torch.exp(self.log_std)\n",
        "        value = self.critic(state)\n",
        "        return action_mean, action_std, value\n",
        "\n",
        "# PPO agent\n",
        "class PPO:\n",
        "    def __init__(self, state_dim, action_dim, env):\n",
        "        self.actor_critic = ActorCritic(state_dim, action_dim)\n",
        "        self.optimizer = optim.Adam(self.actor_critic.parameters(), lr=3e-4)\n",
        "        self.env = env\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "\n",
        "        self.clip_epsilon = 0.2\n",
        "        self.gamma = 0.99\n",
        "        self.lmbda = 0.95\n",
        "        self.epochs = 10\n",
        "        self.batch_size = 64\n",
        "\n",
        "    def select_action(self, state):\n",
        "        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
        "        action_mean, action_std, _ = self.actor_critic(state)\n",
        "        action_dist = MultivariateNormal(action_mean, torch.diag(action_std ** 2))\n",
        "        action = action_dist.sample()\n",
        "        action_log_prob = action_dist.log_prob(action)\n",
        "        return action.detach().numpy().flatten(), action_log_prob.item()\n",
        "\n",
        "    def compute_gae(self, rewards, values, dones):\n",
        "        values = values + [0]\n",
        "        gae = 0\n",
        "        returns = []\n",
        "        for step in reversed(range(len(rewards))):\n",
        "            delta = rewards[step] + self.gamma * values[step + 1] * (1 - dones[step]) - values[step]\n",
        "            gae = delta + self.gamma * self.lmbda * (1 - dones[step]) * gae\n",
        "            returns.insert(0, gae + values[step])\n",
        "        return returns\n",
        "\n",
        "    def update(self, memory):\n",
        "        states, actions, log_probs, rewards, next_states, dones = zip(*memory)\n",
        "        states = torch.tensor(states, dtype=torch.float32)\n",
        "        actions = torch.tensor(actions, dtype=torch.float32)\n",
        "        log_probs = torch.tensor(log_probs, dtype=torch.float32)\n",
        "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
        "        dones = torch.tensor(dones, dtype=torch.float32)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            _, _, values = self.actor_critic(states)\n",
        "            values = values.squeeze().numpy()\n",
        "            returns = self.compute_gae(rewards, values, dones)\n",
        "\n",
        "        returns = torch.tensor(returns, dtype=torch.float32)\n",
        "        advantages = returns - torch.tensor(values, dtype=torch.float32)\n",
        "\n",
        "        for _ in range(self.epochs):\n",
        "            for i in range(0, len(memory), self.batch_size):\n",
        "                batch_states = states[i:i + self.batch_size]\n",
        "                batch_actions = actions[i:i + self.batch_size]\n",
        "                batch_log_probs = log_probs[i:i + self.batch_size]\n",
        "                batch_advantages = advantages[i:i + self.batch_size]\n",
        "                batch_returns = returns[i:i + self.batch_size]\n",
        "\n",
        "                action_mean, action_std, values = self.actor_critic(batch_states)\n",
        "                action_dist = MultivariateNormal(action_mean, torch.diag(action_std ** 2))\n",
        "                new_log_probs = action_dist.log_prob(batch_actions)\n",
        "\n",
        "                ratios = torch.exp(new_log_probs - batch_log_probs)\n",
        "                surrogate1 = ratios * batch_advantages\n",
        "                surrogate2 = torch.clamp(ratios, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * batch_advantages\n",
        "                policy_loss = -torch.min(surrogate1, surrogate2).mean()\n",
        "\n",
        "                value_loss = (batch_returns - values.squeeze()).pow(2).mean()\n",
        "\n",
        "                loss = policy_loss + 0.5 * value_loss\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "    def train(self, max_episodes=1000):\n",
        "        for episode in range(max_episodes):\n",
        "            state = self.env.reset()\n",
        "            memory = []\n",
        "            episode_reward = 0\n",
        "\n",
        "            done = False\n",
        "            while not done:\n",
        "                action, action_log_prob = self.select_action(state)\n",
        "                next_state, reward, done, truncated, _ = self.env.step(action)\n",
        "                memory.append((state, action, action_log_prob, reward, next_state, done or truncated))\n",
        "                state = next_state\n",
        "                episode_reward += reward\n",
        "\n",
        "            self.update(memory)\n",
        "\n",
        "            print(f\"Episode {episode}, Reward: {episode_reward}\")\n",
        "\n",
        "env = gym.make(\"Pendulum-v1\", new_step_api=True)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "\n",
        "agent = PPO(state_dim, action_dim, env)\n",
        "agent.train()"
      ]
    }
  ]
}