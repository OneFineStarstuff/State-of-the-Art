{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyO74mHacLKS82KYtw3V3TUB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/State-of-the-Art/blob/main/Reinforcement_Learning_(RL).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "keyy3XiDRBKX"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Initialize parameters\n",
        "num_states = 5  # Total number of states\n",
        "num_actions = 2  # Number of possible actions (left or right)\n",
        "q_table = np.zeros((num_states, num_actions))  # Q-table initialized to zero\n",
        "learning_rate = 0.1  # How much new information overrides old information\n",
        "discount_factor = 0.9  # How much future rewards are valued compared to immediate rewards\n",
        "epsilon = 0.1  # Initial exploration rate\n",
        "\n",
        "# Dummy environment function\n",
        "def step(state, action):\n",
        "    if action == 0:  # Move left\n",
        "        next_state = max(0, state - 1)\n",
        "    else:  # Move right\n",
        "        next_state = min(num_states - 1, state + 1)\n",
        "    # Reward is +1 for reaching the final state, -1 otherwise\n",
        "    reward = 1 if next_state == num_states - 1 else -1\n",
        "    return next_state, reward\n",
        "\n",
        "# Q-learning training loop\n",
        "for episode in range(1000):  # Run for 1000 episodes\n",
        "    state = np.random.randint(0, num_states)  # Start from a random state\n",
        "    while state != num_states - 1:  # Continue until reaching the final state\n",
        "        # Choose action based on epsilon-greedy policy\n",
        "        if np.random.rand() < epsilon:\n",
        "            action = np.random.randint(0, num_actions)  # Explore: random action\n",
        "        else:\n",
        "            action = np.argmax(q_table[state])  # Exploit: best known action\n",
        "\n",
        "        # Take action and observe result\n",
        "        next_state, reward = step(state, action)\n",
        "\n",
        "        # Update Q-value using the Bellman equation\n",
        "        q_table[state, action] = (1 - learning_rate) * q_table[state, action] + \\\n",
        "                                 learning_rate * (reward + discount_factor * np.max(q_table[next_state]))\n",
        "\n",
        "        # Transition to the next state\n",
        "        state = next_state\n",
        "\n",
        "    # Decay epsilon over time for less exploration and more exploitation\n",
        "    epsilon = max(0.01, epsilon * 0.99)\n",
        "\n",
        "    # Log progress periodically\n",
        "    if episode % 100 == 0:\n",
        "        print(f\"Episode {episode}, Q-table:\")\n",
        "        print(q_table)\n",
        "\n",
        "# Output the final Q-table\n",
        "print(\"Trained Q-table:\")\n",
        "print(q_table)\n",
        "\n",
        "# Test the optimal policy after training\n",
        "print(\"\\nTesting optimal policy:\")\n",
        "state = 0\n",
        "while state != num_states - 1:\n",
        "    action = np.argmax(q_table[state])  # Choose the best action for the current state\n",
        "    next_state, reward = step(state, action)\n",
        "    print(f\"State: {state}, Action: {action}, Next State: {next_state}, Reward: {reward}\")\n",
        "    state = next_state"
      ]
    }
  ]
}