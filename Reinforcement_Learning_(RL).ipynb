{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMUsCM1efi4iTAdgbiuR5yT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/State-of-the-Art/blob/main/Reinforcement_Learning_(RL).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bg34fTxyBaAW"
      },
      "outputs": [],
      "source": [
        "pip install gym"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "from collections import deque\n",
        "import gym\n",
        "\n",
        "# Define the DQN class\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(DQN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 128)  # First fully connected layer\n",
        "        self.fc2 = nn.Linear(128, 128)  # Second fully connected layer\n",
        "        self.fc3 = nn.Linear(128, output_dim)  # Output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))  # Apply ReLU activation after the first layer\n",
        "        x = torch.relu(self.fc2(x))  # Apply ReLU activation after the second layer\n",
        "        return self.fc3(x)  # Output layer\n",
        "\n",
        "# Define the ReplayMemory class\n",
        "class ReplayMemory:\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = deque(maxlen=capacity)  # Initialize the replay buffer with a maximum capacity\n",
        "\n",
        "    def push(self, transition):\n",
        "        self.memory.append(transition)  # Add a transition to the replay buffer\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)  # Sample a batch of transitions\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)  # Return the current size of the replay buffer\n",
        "\n",
        "# Define the optimize_model function\n",
        "def optimize_model(policy_net, target_net, memory, optimizer, batch_size, gamma):\n",
        "    if len(memory) < batch_size:\n",
        "        return  # Do not optimize if there are not enough samples in the replay buffer\n",
        "    transitions = memory.sample(batch_size)  # Sample a batch of transitions\n",
        "\n",
        "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for detailed explanation).\n",
        "    batch = list(zip(*transitions))\n",
        "\n",
        "    state_batch = torch.cat(batch[0])\n",
        "    action_batch = torch.cat(batch[1]).view(-1, 1)  # Ensure action_batch is a 2D tensor\n",
        "    reward_batch = torch.cat(batch[2]).view(-1, 1)  # Ensure reward_batch is a 2D tensor\n",
        "    next_state_batch = torch.cat(batch[3])\n",
        "    done_batch = torch.cat(batch[4]).view(-1, 1)  # Ensure done_batch is a 2D tensor\n",
        "\n",
        "    # Verify dimensions of the tensors\n",
        "    print(f\"state_batch.shape: {state_batch.shape}\")\n",
        "    print(f\"action_batch.shape: {action_batch.shape}\")\n",
        "    print(f\"reward_batch.shape: {reward_batch.shape}\")\n",
        "    print(f\"next_state_batch.shape: {next_state_batch.shape}\")\n",
        "    print(f\"done_batch.shape: {done_batch.shape}\")\n",
        "\n",
        "    state_action_values = policy_net(state_batch).gather(1, action_batch)  # Get Q-values for the selected actions\n",
        "    next_state_values = target_net(next_state_batch).max(1)[0].view(-1, 1).detach()  # Get maximum Q-value for the next states\n",
        "    expected_state_action_values = reward_batch + (gamma * next_state_values * (1 - done_batch))  # Calculate expected Q-values\n",
        "\n",
        "    loss = nn.functional.smooth_l1_loss(state_action_values, expected_state_action_values)  # Compute the loss\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()  # Backpropagate the loss\n",
        "    optimizer.step()  # Update the model parameters\n",
        "\n",
        "# Define a function to select an action based on an epsilon-greedy policy\n",
        "def select_action(policy_net, state, epsilon):\n",
        "    if random.random() < epsilon:\n",
        "        return torch.tensor([[random.randrange(policy_net.fc3.out_features)]], dtype=torch.long)  # Random action\n",
        "    else:\n",
        "        with torch.no_grad():\n",
        "            return policy_net(state).max(1)[1].view(1, 1)  # Action with highest Q-value\n",
        "\n",
        "# Set up the environment\n",
        "env = gym.make('CartPole-v1')  # Example environment from OpenAI Gym\n",
        "\n",
        "# Example usage\n",
        "policy_net = DQN(input_dim=env.observation_space.shape[0], output_dim=env.action_space.n)\n",
        "target_net = DQN(input_dim=env.observation_space.shape[0], output_dim=env.action_space.n)\n",
        "target_net.load_state_dict(policy_net.state_dict())  # Copy the weights from policy_net to target_net\n",
        "target_net.eval()  # Set target_net to evaluation mode\n",
        "\n",
        "memory = ReplayMemory(10000)  # Initialize the replay memory with a capacity of 10,000\n",
        "optimizer = optim.RMSprop(policy_net.parameters())  # Define the optimizer\n",
        "batch_size = 32\n",
        "gamma = 0.99\n",
        "epsilon = 0.1  # Epsilon for epsilon-greedy policy\n",
        "\n",
        "state = env.reset()\n",
        "state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)  # Convert state to tensor\n",
        "for t in range(1000):\n",
        "    action = select_action(policy_net, state, epsilon)\n",
        "    next_state, reward, done, _ = env.step(action.item())\n",
        "    next_state = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0)  # Convert next state to tensor\n",
        "    reward = torch.tensor([reward], dtype=torch.float32).view(-1, 1)  # Ensure reward is a 2D tensor\n",
        "    done = torch.tensor([done], dtype=torch.float32).view(-1, 1)  # Ensure done is a 2D tensor\n",
        "\n",
        "    memory.push((state, action, reward, next_state, done))  # Add the transition to the replay memory\n",
        "    state = next_state\n",
        "    optimize_model(policy_net, target_net, memory, optimizer, batch_size, gamma)  # Optimize the model\n",
        "    if done:\n",
        "        state = env.reset()  # Reset the environment if the episode is done\n",
        "        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)  # Convert state to tensor"
      ],
      "metadata": {
        "id": "FKt23AHmCCx1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}