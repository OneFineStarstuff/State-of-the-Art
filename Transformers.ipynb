{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNkRjWQujHL32p57yrPyH7+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/State-of-the-Art/blob/main/Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torchtext"
      ],
      "metadata": {
        "id": "SWm4lyzl4nPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show torch\n",
        "!pip show torchtext"
      ],
      "metadata": {
        "id": "LLnCHNIJ5UGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WazM6Ccf32D7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# Define a simple dataset\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, text_data, tokenizer, vocab, pad_token):\n",
        "        self.data = [torch.tensor(vocab(tokenizer(line)), dtype=torch.long) for line in text_data]\n",
        "        self.tokenizer = tokenizer\n",
        "        self.vocab = vocab\n",
        "        self.pad_token = pad_token\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "        max_len = max(len(item) for item in batch)\n",
        "        batch = [torch.cat([item, torch.tensor([self.pad_token] * (max_len - len(item)), dtype=torch.long)]) for item in batch]\n",
        "        return torch.stack(batch)\n",
        "\n",
        "# Define the Transformer model\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, num_heads, num_encoder_layers, num_decoder_layers, dim_feedforward):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=embed_size,\n",
        "            nhead=num_heads,\n",
        "            num_encoder_layers=num_encoder_layers,\n",
        "            num_decoder_layers=num_decoder_layers,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "        )\n",
        "        self.fc_out = nn.Linear(embed_size, vocab_size)\n",
        "\n",
        "    def generate_square_subsequent_mask(self, sz):\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return mask\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src = self.embedding(src)\n",
        "        tgt = self.embedding(tgt)\n",
        "        src_mask = self.generate_square_subsequent_mask(src.size(0))\n",
        "        tgt_mask = self.generate_square_subsequent_mask(tgt.size(0))\n",
        "        memory_mask = self.generate_square_subsequent_mask(tgt.size(0))\n",
        "        output = self.transformer(src, tgt, src_mask=src_mask, tgt_mask=tgt_mask, memory_mask=memory_mask)\n",
        "        return self.fc_out(output)\n",
        "\n",
        "# Tokenization and vocabulary\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "train_data = [\"hello world\", \"transformer model example\", \"deep learning with transformers\"]\n",
        "vocab = build_vocab_from_iterator(map(tokenizer, train_data), specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
        "vocab.set_default_index(vocab['<unk>'])\n",
        "\n",
        "# Create dataset and dataloader\n",
        "pad_token = vocab['<pad>']\n",
        "dataset = TextDataset(train_data, tokenizer, vocab, pad_token)\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=dataset.collate_fn)\n",
        "\n",
        "# Model, loss, and optimizer\n",
        "model = TransformerModel(vocab_size=len(vocab), embed_size=128, num_heads=2, num_encoder_layers=2, num_decoder_layers=2, dim_feedforward=512)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(10):\n",
        "    for batch in dataloader:\n",
        "        src = batch[:, :-1]\n",
        "        tgt = batch[:, 1:]\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, src)\n",
        "        loss = criterion(output.view(-1, len(vocab)), tgt.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}')"
      ]
    }
  ]
}