{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPy+JC4oJZVfGE7ujxOqi77",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/State-of-the-Art/blob/main/DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import gym\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "\n",
        "# Define the neural network\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(DQN, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.fc3 = nn.Linear(128, action_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "# Replay memory to store transitions\n",
        "class ReplayMemory:\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, transition):\n",
        "        state, action, reward, next_state, done = transition\n",
        "        if isinstance(state, np.ndarray) and state.shape == (4,) and \\\n",
        "           isinstance(next_state, np.ndarray) and next_state.shape == (4,):\n",
        "            self.memory.append(transition)\n",
        "        else:\n",
        "            print(f\"Invalid transition: {transition}\")\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "# Initialize environment and model\n",
        "env = gym.make('CartPole-v1', new_step_api=True)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.n\n",
        "model = DQN(state_dim, action_dim)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.MSELoss()\n",
        "memory = ReplayMemory(10000)\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 64\n",
        "gamma = 0.99\n",
        "epsilon_start = 1.0\n",
        "epsilon_end = 0.01\n",
        "epsilon_decay = 500\n",
        "\n",
        "def epsilon_by_frame(frame_idx):\n",
        "    return epsilon_end + (epsilon_start - epsilon_end) * np.exp(-1. * frame_idx / epsilon_decay)\n",
        "\n",
        "# Training loop\n",
        "num_episodes = 500\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()  # `env.reset()` returns a single value\n",
        "    state = np.array(state)  # Ensure state is a NumPy array\n",
        "    total_reward = 0\n",
        "    for t in range(200):\n",
        "        epsilon = epsilon_by_frame(t)\n",
        "        if random.random() > epsilon:\n",
        "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "            print(f'state_tensor shape: {state_tensor.shape}')  # Debugging print\n",
        "            action = model(state_tensor).max(1)[1].item()\n",
        "        else:\n",
        "            action = env.action_space.sample()\n",
        "\n",
        "        next_state, reward, done, truncated, _ = env.step(action)\n",
        "        next_state = np.array(next_state)  # Ensure next_state is a NumPy array\n",
        "        memory.push((state, action, reward, next_state, done or truncated))\n",
        "\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "        if done or truncated:\n",
        "            break\n",
        "\n",
        "        if len(memory) > batch_size:\n",
        "            transitions = memory.sample(batch_size)\n",
        "            batch_state, batch_action, batch_reward, batch_next_state, batch_done = zip(*transitions)\n",
        "\n",
        "            # Debugging print\n",
        "            print(f'batch_state shapes: {[np.array(state).shape for state in batch_state]}')\n",
        "\n",
        "            batch_state = torch.FloatTensor(np.array(batch_state))\n",
        "            batch_action = torch.LongTensor(np.array(batch_action)).unsqueeze(1)\n",
        "            batch_reward = torch.FloatTensor(np.array(batch_reward))\n",
        "            batch_next_state = torch.FloatTensor(np.array(batch_next_state))\n",
        "            batch_done = torch.FloatTensor(np.array(batch_done))\n",
        "\n",
        "            current_q_values = model(batch_state).gather(1, batch_action)\n",
        "            max_next_q_values = model(batch_next_state).max(1)[0]\n",
        "            expected_q_values = batch_reward + (gamma * max_next_q_values * (1 - batch_done))\n",
        "\n",
        "            loss = criterion(current_q_values, expected_q_values.unsqueeze(1))\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    print(f\"Episode {episode + 1}, Total Reward: {total_reward}\")\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "state = env.reset()  # `env.reset()` returns a single value\n",
        "state = np.array(state)  # Ensure state is a NumPy array\n",
        "total_reward = 0\n",
        "for t in range(200):\n",
        "    state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "    print(f'state_tensor shape: {state_tensor.shape}')  # Debugging print during evaluation\n",
        "    action = model(state_tensor).max(1)[1].item()\n",
        "    next_state, reward, done, truncated, _ = env.step(action)\n",
        "    next_state = np.array(next_state)  # Ensure next_state is a NumPy array\n",
        "    state = next_state\n",
        "    total_reward += reward\n",
        "    if done or truncated:\n",
        "        break\n",
        "print(f\"Total Reward (Evaluation): {total_reward}\")"
      ],
      "metadata": {
        "id": "P2mbv_VOkVVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import gym\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "\n",
        "# Define the neural network\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(DQN, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.fc3 = nn.Linear(128, action_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "# Replay memory to store transitions\n",
        "class ReplayMemory:\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, transition):\n",
        "        self.memory.append(transition)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "# Initialize environment and model\n",
        "env = gym.make('CartPole-v1')\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.n\n",
        "model = DQN(state_dim, action_dim)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.MSELoss()\n",
        "memory = ReplayMemory(10000)\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 64\n",
        "gamma = 0.99\n",
        "epsilon_start = 1.0\n",
        "epsilon_end = 0.01\n",
        "epsilon_decay = 500\n",
        "\n",
        "def epsilon_by_frame(frame_idx):\n",
        "    return epsilon_end + (epsilon_start - epsilon_end) * np.exp(-1. * frame_idx / epsilon_decay)\n",
        "\n",
        "# Training loop\n",
        "num_episodes = 500\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "    for t in range(200):\n",
        "        epsilon = epsilon_by_frame(t)\n",
        "        if random.random() > epsilon:\n",
        "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "            action = model(state_tensor).max(1)[1].item()\n",
        "        else:\n",
        "            action = env.action_space.sample()\n",
        "\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        memory.push((state, action, reward, next_state, done))\n",
        "\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "        if len(memory) > batch_size:\n",
        "            # Sample a batch of transitions from the replay memory\n",
        "            transitions = memory.sample(batch_size)\n",
        "            batch_state, batch_action, batch_reward, batch_next_state, batch_done = zip(*transitions)\n",
        "\n",
        "            # Convert to PyTorch tensors\n",
        "            batch_state = torch.FloatTensor(batch_state)\n",
        "            batch_action = torch.LongTensor(batch_action).unsqueeze(1)\n",
        "            batch_reward = torch.FloatTensor(batch_reward).unsqueeze(1)\n",
        "            batch_next_state = torch.FloatTensor(batch_next_state)\n",
        "            batch_done = torch.FloatTensor(batch_done).unsqueeze(1)\n",
        "\n",
        "            # Compute the Q values for the current state-action pairs\n",
        "            q_values = model(batch_state).gather(1, batch_action)\n",
        "\n",
        "            # Compute the target Q values\n",
        "            next_q_values = model(batch_next_state).max(1)[0].detach().unsqueeze(1)\n",
        "            target_q_values = batch_reward + (gamma * next_q_values * (1 - batch_done))\n",
        "\n",
        "            # Compute the loss\n",
        "            loss = criterion(q_values, target_q_values)\n",
        "\n",
        "            # Optimize the model\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    print(f\"Episode {episode + 1}, Total Reward: {total_reward}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    # If the total reward exceeds a threshold, stop training\n",
        "    if total_reward > 195:\n",
        "        print(f\"Solved in {episode + 1} episodes!\")\n",
        "        break\n",
        "\n",
        "# Test the trained model\n",
        "state = env.reset()\n",
        "total_reward = 0\n",
        "for t in range(200):\n",
        "    env.render()\n",
        "    state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "    action = model(state_tensor).max(1)[1].item()\n",
        "    next_state, reward, done, _ = env.step(action)\n",
        "    total_reward += reward\n",
        "    state = next_state\n",
        "    if done:\n",
        "        break\n",
        "\n",
        "print(f\"Test Reward: {total_reward}\")\n",
        "env.close()"
      ],
      "metadata": {
        "id": "yRxgqMc-s2C5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}