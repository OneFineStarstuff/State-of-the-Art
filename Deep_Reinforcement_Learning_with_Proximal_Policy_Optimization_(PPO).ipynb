{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyO30rLRtyT8rIWAP1DH6Zfm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/State-of-the-Art/blob/main/Deep_Reinforcement_Learning_with_Proximal_Policy_Optimization_(PPO).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RsStg4l8QFcV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import gym\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, input_dim, action_dim):\n",
        "        super(ActorCritic, self).__init__()\n",
        "        self.actor = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, action_dim),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "        self.critic = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        action_probs = self.actor(x)\n",
        "        state_value = self.critic(x)\n",
        "        return action_probs, state_value\n",
        "\n",
        "# Hyperparameters\n",
        "gamma = 0.99\n",
        "clip_epsilon = 0.2\n",
        "critic_coef = 0.5\n",
        "entropy_coef = 0.01\n",
        "lr = 3e-4\n",
        "num_episodes = 1000\n",
        "update_epochs = 4\n",
        "batch_size = 64\n",
        "\n",
        "# Environment setup\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "input_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.n\n",
        "\n",
        "model = ActorCritic(input_dim, action_dim)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "# Storage for trajectories\n",
        "class RolloutBuffer:\n",
        "    def __init__(self):\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.logprobs = []\n",
        "        self.rewards = []\n",
        "        self.dones = []\n",
        "        self.values = []\n",
        "\n",
        "    def clear(self):\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.logprobs = []\n",
        "        self.rewards = []\n",
        "        self.dones = []\n",
        "        self.values = []\n",
        "\n",
        "buffer = RolloutBuffer()\n",
        "\n",
        "# Advantage calculation\n",
        "def compute_advantages(rewards, dones, values, next_value, gamma):\n",
        "    advantages = []\n",
        "    gae = 0\n",
        "    for reward, done, value in zip(reversed(rewards), reversed(dones), reversed(values)):\n",
        "        gae = reward + gamma * (1 - done) * gae - value\n",
        "        advantages.insert(0, gae)\n",
        "    returns = [a + v for a, v in zip(advantages, values)]\n",
        "    return torch.tensor(advantages), torch.tensor(returns)\n",
        "\n",
        "# Training loop\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    buffer.clear()\n",
        "    episode_reward = 0\n",
        "\n",
        "    for t in range(200):  # Limit steps per episode\n",
        "        state_tensor = torch.tensor(state, dtype=torch.float32)\n",
        "        action_probs, value = model(state_tensor)\n",
        "\n",
        "        # Sample action\n",
        "        dist = Categorical(action_probs)\n",
        "        action = dist.sample()\n",
        "        log_prob = dist.log_prob(action)\n",
        "\n",
        "        # Step environment\n",
        "        next_state, reward, done, _ = env.step(action.item())\n",
        "        episode_reward += reward\n",
        "\n",
        "        # Store experience\n",
        "        buffer.states.append(state)\n",
        "        buffer.actions.append(action)\n",
        "        buffer.logprobs.append(log_prob)\n",
        "        buffer.rewards.append(reward)\n",
        "        buffer.dones.append(done)\n",
        "        buffer.values.append(value.item())\n",
        "\n",
        "        state = next_state\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # Compute advantages and returns\n",
        "    _, next_value = model(torch.tensor(next_state, dtype=torch.float32))\n",
        "    advantages, returns = compute_advantages(\n",
        "        buffer.rewards, buffer.dones, buffer.values, next_value.item(), gamma\n",
        "    )\n",
        "\n",
        "    # PPO Update\n",
        "    for _ in range(update_epochs):\n",
        "        for i in range(0, len(buffer.states), batch_size):\n",
        "            # Batch samples\n",
        "            states = torch.tensor(buffer.states[i:i+batch_size], dtype=torch.float32)\n",
        "            actions = torch.tensor(buffer.actions[i:i+batch_size])\n",
        "            logprobs = torch.stack(buffer.logprobs[i:i+batch_size])\n",
        "            advs = advantages[i:i+batch_size]\n",
        "            rets = returns[i:i+batch_size]\n",
        "\n",
        "            # Forward pass\n",
        "            new_action_probs, new_values = model(states)\n",
        "            new_dist = Categorical(new_action_probs)\n",
        "            new_logprobs = new_dist.log_prob(actions)\n",
        "\n",
        "            # Policy ratio\n",
        "            ratios = torch.exp(new_logprobs - logprobs.detach())\n",
        "\n",
        "            # Clipped objective\n",
        "            clipped_advs = torch.clamp(ratios, 1 - clip_epsilon, 1 + clip_epsilon) * advs\n",
        "            policy_loss = -torch.min(ratios * advs, clipped_advs).mean()\n",
        "\n",
        "            # Value function loss\n",
        "            value_loss = nn.MSELoss()(new_values.squeeze(), rets)\n",
        "\n",
        "            # Entropy for exploration\n",
        "            entropy_loss = -new_dist.entropy().mean()\n",
        "\n",
        "            # Total loss\n",
        "            loss = policy_loss + critic_coef * value_loss + entropy_coef * entropy_loss\n",
        "\n",
        "            # Optimize model\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    print(f\"Episode {episode + 1}, Reward: {episode_reward}\")\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import gym\n",
        "\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, input_dim, action_dim):\n",
        "        super(ActorCritic, self).__init__()\n",
        "        self.actor = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, action_dim),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "        self.critic = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        action_probs = self.actor(x)\n",
        "        state_value = self.critic(x)\n",
        "        return action_probs, state_value\n",
        "\n",
        "# Environment setup with new API enabled\n",
        "env = gym.make('CartPole-v1', new_step_api=True)\n",
        "\n",
        "model = ActorCritic(input_dim=env.observation_space.shape[0], action_dim=env.action_space.n)\n",
        "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
        "\n",
        "state, _ = env.reset()  # Reset environment for new API\n",
        "for t in range(200):\n",
        "    state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
        "    action_probs, _ = model(state_tensor)\n",
        "    action = torch.argmax(action_probs).item()\n",
        "\n",
        "    # Step environment\n",
        "    next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "    done = terminated or truncated  # Combine flags for done\n",
        "\n",
        "    if done:\n",
        "        break\n",
        "\n",
        "    state = next_state\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "id": "v2vN4eCfQvGP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}