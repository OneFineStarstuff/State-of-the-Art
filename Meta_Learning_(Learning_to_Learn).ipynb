{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyO5k3lk+ggPtTdN/xEU9Byt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/State-of-the-Art/blob/main/Meta_Learning_(Learning_to_Learn).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESU02dneqZW7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "\n",
        "# Simple regression model\n",
        "class RegressionModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RegressionModel, self).__init__()\n",
        "        self.fc = nn.Linear(1, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "def maml_update(model, loss_fn, x, y, lr_inner):\n",
        "    # Compute loss and gradients for the task\n",
        "    loss = loss_fn(model(x), y)\n",
        "    gradients = torch.autograd.grad(loss, model.parameters(), create_graph=True)\n",
        "\n",
        "    # Update model parameters manually with the computed gradients\n",
        "    updated_params = {}\n",
        "    for (name, param), grad in zip(model.named_parameters(), gradients):\n",
        "        updated_params[name] = param - lr_inner * grad\n",
        "\n",
        "    return updated_params\n",
        "\n",
        "def apply_updated_params(model, updated_params):\n",
        "    # Create a temporary model with updated parameters\n",
        "    temp_model = RegressionModel()\n",
        "    temp_model.load_state_dict(model.state_dict())  # Copy original model state\n",
        "    for name, param in temp_model.named_parameters():\n",
        "        param.data.copy_(updated_params[name].data)\n",
        "    return temp_model\n",
        "\n",
        "# Outer loop: Meta-learning step\n",
        "def meta_learning_step(model, optimizer, tasks, loss_fn, lr_inner):\n",
        "    meta_loss = 0\n",
        "\n",
        "    for x, y in tasks:\n",
        "        # Perform inner loop (fine-tuning) and get updated parameters\n",
        "        updated_params = maml_update(model, loss_fn, x, y, lr_inner)\n",
        "        task_model = apply_updated_params(model, updated_params)\n",
        "\n",
        "        # Evaluate task-specific model on the task\n",
        "        task_loss = loss_fn(task_model(x), y)\n",
        "        meta_loss += task_loss\n",
        "\n",
        "    # Update the original model's parameters based on the meta-loss\n",
        "    optimizer.zero_grad()\n",
        "    meta_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return meta_loss.item()\n",
        "\n",
        "# Initialize model, optimizer, and loss function\n",
        "model = RegressionModel()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "# Example usage: updating the model for a set of tasks\n",
        "tasks = [(torch.tensor([[1.0], [2.0]]), torch.tensor([[3.0], [5.0]])),\n",
        "         (torch.tensor([[3.0], [4.0]]), torch.tensor([[7.0], [9.0]]))]\n",
        "\n",
        "# Meta-training loop\n",
        "epochs = 100\n",
        "lr_inner = 0.01  # Learning rate for inner loop\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    meta_loss = meta_learning_step(model, optimizer, tasks, loss_fn, lr_inner)\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch}, Meta-Loss: {meta_loss:.4f}\")"
      ]
    }
  ]
}