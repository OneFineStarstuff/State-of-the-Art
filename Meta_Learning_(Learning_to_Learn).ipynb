{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyODPxnB5s2MSWtBGi5djo0k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/State-of-the-Art/blob/main/Meta_Learning_(Learning_to_Learn).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OS5SpRr3ARak"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import OrderedDict\n",
        "\n",
        "# Define a simple feedforward neural network\n",
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(SimpleModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        return self.fc2(x)\n",
        "\n",
        "# MAML framework\n",
        "class MAML:\n",
        "    def __init__(self, model, lr_inner=0.01, lr_outer=0.001, num_adapt_steps=1):\n",
        "        self.model = model\n",
        "        self.lr_inner = lr_inner  # Inner loop learning rate\n",
        "        self.lr_outer = lr_outer  # Outer loop learning rate\n",
        "        self.num_adapt_steps = num_adapt_steps  # Steps in the inner loop\n",
        "        self.outer_optimizer = optim.Adam(self.model.parameters(), lr=self.lr_outer)\n",
        "\n",
        "    def adapt(self, loss):\n",
        "        # Compute gradients for inner loop updates\n",
        "        grads = torch.autograd.grad(loss, self.model.parameters(), create_graph=True)\n",
        "        updated_params = OrderedDict()\n",
        "        for (name, param), grad in zip(self.model.named_parameters(), grads):\n",
        "            updated_params[name] = param - self.lr_inner * grad\n",
        "        return updated_params\n",
        "\n",
        "    def forward_with_params(self, x, params):\n",
        "        # Use updated parameters for forward pass\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if name in params:\n",
        "                param.data.copy_(params[name])\n",
        "        return self.model(x)\n",
        "\n",
        "    def meta_update(self, meta_loss):\n",
        "        # Backpropagate and update the meta-parameters (outer loop)\n",
        "        self.outer_optimizer.zero_grad()\n",
        "        meta_loss.backward()\n",
        "        self.outer_optimizer.step()\n",
        "\n",
        "# Sinusoidal regression task data generator\n",
        "def generate_sinusoidal_data(batch_size, amplitude_range=(0.1, 5.0), phase_range=(0, 3.14)):\n",
        "    amplitudes = torch.rand(batch_size) * (amplitude_range[1] - amplitude_range[0]) + amplitude_range[0]\n",
        "    phases = torch.rand(batch_size) * (phase_range[1] - phase_range[0]) + phase_range[0]\n",
        "    x = torch.linspace(-5, 5, 100).unsqueeze(0).repeat(batch_size, 1)  # (batch_size, num_points)\n",
        "    y = amplitudes.unsqueeze(1) * torch.sin(x + phases.unsqueeze(1))  # (batch_size, num_points)\n",
        "    return x.view(-1, 1), y.view(-1, 1)  # Reshape to (batch_size * num_points, input_dim)\n",
        "\n",
        "# Training loop\n",
        "input_dim = 1  # Input dimension\n",
        "hidden_dim = 40  # Hidden layer dimension\n",
        "output_dim = 1  # Output dimension\n",
        "meta_batch_size = 32  # Number of tasks in each meta-update\n",
        "task_data_points = 10  # Number of data points per task\n",
        "num_meta_epochs = 100  # Number of meta-epochs\n",
        "\n",
        "# Initialize model and MAML framework\n",
        "model = SimpleModel(input_dim, hidden_dim, output_dim)\n",
        "maml = MAML(model)\n",
        "\n",
        "for epoch in range(num_meta_epochs):\n",
        "    meta_loss = 0\n",
        "    for _ in range(meta_batch_size):\n",
        "        # Generate task-specific data\n",
        "        x, y = generate_sinusoidal_data(1)  # One task (batch size = 1)\n",
        "        x_train, y_train = x[:task_data_points], y[:task_data_points]  # Training set\n",
        "        x_val, y_val = x[task_data_points:], y[task_data_points:]  # Validation set\n",
        "\n",
        "        # Inner loop: Adapt to task\n",
        "        outputs_train = maml.model(x_train)\n",
        "        inner_loss = nn.MSELoss()(outputs_train, y_train)\n",
        "        task_params = maml.adapt(inner_loss)\n",
        "\n",
        "        # Outer loop: Evaluate on validation set\n",
        "        outputs_val = maml.forward_with_params(x_val, task_params)\n",
        "        meta_loss += nn.MSELoss()(outputs_val, y_val)\n",
        "\n",
        "    # Meta-update (outer loop)\n",
        "    meta_loss /= meta_batch_size  # Average meta-loss across tasks\n",
        "    maml.meta_update(meta_loss)\n",
        "\n",
        "    # Log meta-loss for the epoch\n",
        "    print(f\"Epoch [{epoch + 1}/{num_meta_epochs}], Meta Loss: {meta_loss.item():.4f}\")"
      ]
    }
  ]
}