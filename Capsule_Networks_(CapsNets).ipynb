{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMGssJsbBDwWCmKnwaaTj/H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/State-of-the-Art/blob/main/Capsule_Networks_(CapsNets).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHP1p2lsuzeM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "class CapsuleLayer(nn.Module):\n",
        "    def __init__(self, in_caps, out_caps, in_dim, out_dim):\n",
        "        super(CapsuleLayer, self).__init__()\n",
        "        self.in_caps = in_caps\n",
        "        self.out_caps = out_caps\n",
        "        self.W = nn.Parameter(torch.randn(1, out_caps, in_caps, out_dim, in_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        num_capsules = x.size(1)\n",
        "        in_dim = x.size(2)\n",
        "\n",
        "        W = self.W.expand(batch_size, self.out_caps, self.in_caps, -1, -1)\n",
        "        x = x.unsqueeze(1).unsqueeze(-1)\n",
        "        u_hat = torch.matmul(W, x).squeeze(-1)\n",
        "        u_hat = u_hat.permute(0, 2, 1, 3).contiguous()\n",
        "        return self.squash(u_hat.sum(dim=2))\n",
        "\n",
        "    def squash(self, x):\n",
        "        norm = (x ** 2).sum(dim=-1, keepdim=True)\n",
        "        return (norm / (1 + norm)) * (x / torch.sqrt(norm + 1e-8))\n",
        "\n",
        "class CapsNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CapsNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 256, kernel_size=5, stride=1)\n",
        "        self.conv2 = nn.Conv2d(256, 256, kernel_size=5, stride=2)\n",
        "        self.primary_capsules = nn.ModuleList([nn.Conv2d(256, 8, kernel_size=5, stride=2) for _ in range(32)])\n",
        "        self.digit_capsules = CapsuleLayer(288, 10, 8, 16)\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(288 * 16, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 28 * 28),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        print(\"After conv2:\", x.size())\n",
        "\n",
        "        primary_capsules_output = [caps(x) for caps in self.primary_capsules]\n",
        "        x = torch.cat(primary_capsules_output, dim=1)\n",
        "        print(\"After primary capsules:\", x.size())\n",
        "\n",
        "        batch_size = x.size(0)\n",
        "        num_primary_capsules = x.size(1) // 8 * x.size(2) * x.size(3)\n",
        "        print(\"num_primary_capsules:\", num_primary_capsules)\n",
        "\n",
        "        x = x.view(batch_size, num_primary_capsules, 8)\n",
        "        x = self.digit_capsules(x)\n",
        "        x = x.view(batch_size, -1)  # Flatten for decoder input\n",
        "        print(\"Before decoder:\", x.size())  # Debugging print\n",
        "        x = self.decoder(x)\n",
        "        return x.view(batch_size, 1, 28, 28)\n",
        "\n",
        "# Check device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Initialize model\n",
        "model = CapsNet().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Dummy data for training\n",
        "x = torch.rand(32, 1, 28, 28).to(device)\n",
        "y = torch.rand(32, 1, 28, 28).to(device)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(50):\n",
        "    optimizer.zero_grad()\n",
        "    output = model(x)\n",
        "    loss = criterion(output, y)  # Compute loss with adjusted target size\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")"
      ]
    }
  ]
}