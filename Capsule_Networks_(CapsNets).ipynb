{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNQwIyugAebJzbZqJRa8C0d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/State-of-the-Art/blob/main/Capsule_Networks_(CapsNets).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VGrwHjxNEsQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Squash activation function for capsules\n",
        "def squash(s):\n",
        "    \"\"\"\n",
        "    Squash activation function for capsule networks.\n",
        "    Args:\n",
        "        s: Input tensor (shape [..., output_dim]).\n",
        "    Returns:\n",
        "        s_squashed: Squashed tensor (shape [..., output_dim]).\n",
        "    \"\"\"\n",
        "    s_norm = torch.norm(s, dim=-1, keepdim=True)\n",
        "    return (s_norm / (1 + s_norm ** 2)) * (s / (s_norm + 1e-8))\n",
        "\n",
        "# Dynamic routing between capsules\n",
        "def dynamic_routing(u_hat, num_iterations=3):\n",
        "    \"\"\"\n",
        "    Performs dynamic routing between capsule layers.\n",
        "    Args:\n",
        "        u_hat: Predicted outputs (shape: [batch_size, num_capsules_prev, num_capsules_next, output_dim]).\n",
        "        num_iterations: Number of routing iterations.\n",
        "    Returns:\n",
        "        s: Final routed capsule outputs (shape: [batch_size, num_capsules_next, output_dim]).\n",
        "    \"\"\"\n",
        "    batch_size, num_capsules_prev, num_capsules_next, output_dim = u_hat.size()\n",
        "    b_ij = torch.zeros(batch_size, num_capsules_prev, num_capsules_next).to(u_hat.device)  # Initialize coupling coefficients\n",
        "\n",
        "    for _ in range(num_iterations):\n",
        "        # Softmax over coupling coefficients\n",
        "        c_ij = torch.softmax(b_ij, dim=-1)\n",
        "\n",
        "        # Weighted sum of predictions\n",
        "        s = torch.einsum('bij,bijk->bjk', c_ij, u_hat)\n",
        "        s_squashed = squash(s)  # Apply squash function\n",
        "\n",
        "        # Update coupling coefficients based on agreement\n",
        "        agreement = torch.einsum('bjk,bijk->bij', s_squashed, u_hat)  # Agreement computation\n",
        "        b_ij = b_ij + agreement\n",
        "\n",
        "    return s_squashed\n",
        "\n",
        "# Capsule Layer\n",
        "class CapsuleLayer(nn.Module):\n",
        "    def __init__(self, num_capsules, num_routes, in_channels, out_channels):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            num_capsules: Number of capsules in the layer.\n",
        "            num_routes: Number of input routes (capsules from the previous layer).\n",
        "            in_channels: Dimensionality of the input to each capsule.\n",
        "            out_channels: Dimensionality of the output of each capsule.\n",
        "        \"\"\"\n",
        "        super(CapsuleLayer, self).__init__()\n",
        "        self.num_capsules = num_capsules\n",
        "        self.num_routes = num_routes\n",
        "\n",
        "        # Linear transformations for each capsule\n",
        "        self.capsules = nn.ModuleList(\n",
        "            [nn.Linear(in_channels, out_channels) for _ in range(num_capsules)]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input tensor (shape: [batch_size, num_routes, in_channels]).\n",
        "        Returns:\n",
        "            Final capsule layer outputs after routing (shape: [batch_size, num_capsules, out_channels]).\n",
        "        \"\"\"\n",
        "        # Create predictions (u_hat) for routing\n",
        "        u_hat = torch.stack([capsule(x) for capsule in self.capsules], dim=2)  # [batch_size, num_routes, num_capsules, out_channels]\n",
        "\n",
        "        # Perform dynamic routing\n",
        "        routed_output = dynamic_routing(u_hat, num_iterations=3)\n",
        "        return routed_output\n",
        "\n",
        "# Example usage of the Capsule Layer\n",
        "if __name__ == \"__main__\":\n",
        "    # Example parameters\n",
        "    batch_size = 32\n",
        "    num_routes = 6  # Number of capsules in the previous layer\n",
        "    num_capsules = 10  # Number of capsules in the current layer\n",
        "    in_channels = 8  # Input vector size\n",
        "    out_channels = 16  # Output vector size (per capsule)\n",
        "\n",
        "    # Example input (representing output from a previous capsule layer)\n",
        "    x = torch.randn(batch_size, num_routes, in_channels)  # Shape: [batch_size, num_routes, in_channels]\n",
        "\n",
        "    # Capsule layer instantiation\n",
        "    capsule_layer = CapsuleLayer(num_capsules=num_capsules, num_routes=num_routes, in_channels=in_channels, out_channels=out_channels)\n",
        "\n",
        "    # Forward pass\n",
        "    output = capsule_layer(x)\n",
        "    print(\"Output shape:\", output.shape)  # Expected: [batch_size, num_capsules, out_channels]"
      ]
    }
  ]
}