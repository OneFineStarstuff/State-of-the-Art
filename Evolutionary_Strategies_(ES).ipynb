{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMhJfNKe8BzgtD7bHFx7vDo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/State-of-the-Art/blob/main/Evolutionary_Strategies_(ES).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBzgifRR0nbi"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "# Define the SimpleModel class\n",
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(SimpleModel, self).__init__()\n",
        "        self.fc = nn.Linear(input_dim, output_dim)  # Fully connected layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)  # Apply the linear transformation\n",
        "\n",
        "# Define the evolutionary strategy function\n",
        "def evolutionary_strategy(model, data, labels, population_size=50, generations=100, sigma=0.1, learning_rate=0.01):\n",
        "    criterion = nn.MSELoss()  # Loss function\n",
        "    original_params = torch.nn.utils.parameters_to_vector(model.parameters()).detach()  # Get original parameters\n",
        "\n",
        "    for _ in range(generations):\n",
        "        noise = torch.randn(population_size, original_params.size(0)) * sigma  # Generate noise\n",
        "        rewards = []\n",
        "\n",
        "        for i in range(population_size):\n",
        "            new_params = original_params + noise[i]  # Add noise to original parameters\n",
        "            torch.nn.utils.vector_to_parameters(new_params, model.parameters())  # Set new parameters\n",
        "            with torch.no_grad():\n",
        "                output = model(data)  # Forward pass\n",
        "                loss = criterion(output, labels)  # Compute the loss\n",
        "            rewards.append(-loss.item())  # Negative loss as reward\n",
        "\n",
        "        rewards = torch.tensor(rewards)\n",
        "        normalized_rewards = (rewards - rewards.mean()) / rewards.std()  # Normalize rewards\n",
        "\n",
        "        gradients = torch.matmul(noise.T, normalized_rewards) / (population_size * sigma)  # Calculate gradients\n",
        "        updated_params = original_params + learning_rate * gradients  # Update parameters\n",
        "        torch.nn.utils.vector_to_parameters(updated_params, model.parameters())  # Set updated parameters\n",
        "\n",
        "    print(\"Evolutionary strategy optimization completed.\")\n",
        "\n",
        "# Example usage\n",
        "model = SimpleModel(input_dim=2, output_dim=1)  # Instantiate the model\n",
        "data = torch.randn(100, 2)  # Generate random data\n",
        "labels = torch.randn(100, 1)  # Generate random labels\n",
        "evolutionary_strategy(model, data, labels)  # Perform evolutionary strategy optimization"
      ]
    }
  ]
}